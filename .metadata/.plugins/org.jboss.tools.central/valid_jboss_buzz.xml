<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Consuming Pino logs from Node.js applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/5-kcUwB5T4E/consuming-pino-logs-nodejs-applications" /><author><name>Ash Cripps</name></author><id>1be17e56-397c-4e80-9b83-885fcc6c0ecb</id><updated>2021-10-28T07:00:00Z</updated><published>2021-10-28T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; offers a vast array of options to developers. This is why Red Hat and IBM teamed up to produce the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture&lt;/a&gt;, a series of recommendations to help you build Node.js applications in the cloud. One of our recommendations is that you use &lt;a href="https://getpino.io/"&gt;Pino&lt;/a&gt;, an object logger for Node.js. You can visit &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/logging.md"&gt;this GitHub page&lt;/a&gt; for an overview of how and why to use Pino. This article demonstrates how to create and consume Pino logs with the &lt;a href="https://docs.openshift.com/container-platform/4.7/logging/cluster-logging.html"&gt;Red Hat OpenShift Logging service&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow along, you need a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster and a Node.js application you can deploy to OpenShift. For our example, we'll use the &lt;a href="https://github.com/nodeshift-starters/nodejs-circuit-breaker"&gt;nodejs-circuit-breaker&lt;/a&gt; from &lt;a href="https://github.com/nodeshift"&gt;NodeShift&lt;/a&gt;, a collection of tools maintained by Red Hat for Node.js developers.&lt;/p&gt; &lt;h2&gt;Installing OpenShift Logging&lt;/h2&gt; &lt;p&gt;To deploy OpenShift Logging, we'll install two operators: The OpenShift Elasticsearch Operator and the OpenShift Logging Operator.&lt;/p&gt; &lt;p&gt;To install the OpenShift Elasticsearch Operator:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;In the OpenShift web console, open &lt;strong&gt;OperatorHub &lt;/strong&gt;under the Operators submenu.&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;OpenShift Elasticsearch Operator&lt;/strong&gt; and click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Double-check that the &lt;strong&gt;All namespaces on the cluster&lt;/strong&gt; option is selected.&lt;/li&gt; &lt;li&gt;For an installed namespace, select &lt;strong&gt;openshift-operators-redhat&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select the option to enable recommended monitoring on this namespace.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Wait for the operator to install.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;This operator installs both the &lt;a href="https://www.elastic.co/elasticsearch/"&gt;Elasticsearch&lt;/a&gt; text data store and its &lt;a href="https://www.elastic.co/guide/en/kibana/master/kuery-query.html"&gt;Kibana&lt;/a&gt; visualization tool, which serve as the backbone of the OpenShift Logging system.&lt;/p&gt; &lt;p&gt;After the Elasticsearch Operator is installed, install the OpenShift Logging Operator as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Navigate back to the &lt;strong&gt;OperatorHub&lt;/strong&gt; and select the &lt;strong&gt;OpenShift Logging Operator&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select a specific namespace, then &lt;strong&gt;openshift-logging&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select the option to enable recommended monitoring on this namespace.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Wait for the operator to install.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The key component installed with this operator is the OpenShift Log Forwarder, which sends logs to the Elasticsearch instance. The Log Forwarder takes the container logs from every pod in every namespace and forwards them to the namespace and containers running Elasticsearch. This communication allows the logs to flow where you can analyze them without requiring each container to have a certificate and route set up to access the separate namespace containing Elasticsearch.&lt;/p&gt; &lt;h2&gt;Deploying OpenShift Logging&lt;/h2&gt; &lt;p&gt;Now that you have the building blocks installed via operators, you will deploy the pods containing the logging system. To do this you need a &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"&gt;custom resource definition (CRD)&lt;/a&gt;, a configuration concept in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This CRD defines what and how many pods you need, where to install them, and key setup features for the Elasticsearch instance, such as the size of the disk and the retention policy. The following YAML code is an example CRD for deploying the logging infrastructure:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: "logging.openshift.io/v1" kind: "ClusterLogging" metadata: name: "instance" namespace: "openshift-logging" spec: managementState: "Managed" logStore: type: "elasticsearch" retentionPolicy: application: maxAge: 1d infra: maxAge: 7d audit: maxAge: 7d elasticsearch: nodeCount: 3 storage: storageClassName: size: 200G resources: requests: memory: "8Gi" proxy: resources: limits: memory: 256Mi requests: memory: 256Mi redundancyPolicy: "SingleRedundancy" visualization: type: "kibana" kibana: replicas: 1 curation: type: "curator" curator: schedule: "30 3 * * *" collection: logs: type: "fluentd" fluentd: {}&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: OpenShift Logging is not designed to be a long-term storage solution. This example stores its logs for only seven days before deletion. For long-lived logs, you need to change the &lt;code&gt;retentionPolicy&lt;/code&gt; property and the storage type under &lt;code&gt;storageClassName&lt;/code&gt;. For more information on how to set up suitable storage for long-lived logs, please &lt;a href="https://docs.openshift.com/container-platform/4.8/logging/config/cluster-logging-log-store.html#cluster-logging-elasticsearch-ha_cluster-logging-store"&gt;refer to the documentation.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To create the CRD:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Navigate to &lt;strong&gt;Custom Resource Definitions&lt;/strong&gt; under the &lt;strong&gt;Administration&lt;/strong&gt; tab in the sidebar. Search for "ClusterLogging" and click on the result.&lt;/li&gt; &lt;li&gt;On this page, click on &lt;strong&gt;Actions&lt;/strong&gt; and then &lt;strong&gt;View Instances&lt;/strong&gt; (the page might need a refresh to load). Then click &lt;strong&gt;Create.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Replace the YAML code there with the YAML from the preceding example and click &lt;strong&gt;Create&lt;/strong&gt; again.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;To check the installation's progress, navigate to the pods page. The page should show three Elasticsearch pods spinning up, along with the Kibana pod and some &lt;a href="https://www.fluentd.org/"&gt;Fluentd&lt;/a&gt; pods that support logging. These pods will take a few minutes to spin up.&lt;/p&gt; &lt;h2&gt;Enabling JSON parsing&lt;/h2&gt; &lt;p&gt;As explained at the beginning of this article, we use Pino for logging in our sample Node.js application. To most effectively use the log data generated by Pino, you need to ensure that the OpenShift Logging Operator can parse the JSON data correctly. JSON parsing is possible as of version 5.1 of this operator. You only need to deploy a custom &lt;code&gt;ClusterLogForwarder&lt;/code&gt; resource. This will overwrite the Fluentd pods and provide the configuration needed to parse JSON logs. The configuration is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: logging.openshift.io/v1 kind: ClusterLogForwarder metadata: name: instance namespace: openshift-logging spec: outputDefaults: elasticsearch: structuredTypeKey: kubernetes.pod_name pipelines: - inputRefs: - application - infrastructure - audit name: all-to-default outputRefs: - default parse: json&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;structuredTypeKey&lt;/code&gt; property determines how the new indexes are split up. In this example, the forwarder creates a new index for each pod that has its logs forwarded to Elasticsearch.&lt;/p&gt; &lt;h2&gt;Generating the Node.js logs&lt;/h2&gt; &lt;p&gt;Next, you'll push the application to generate logs from the NodeShift starter repository.&lt;/p&gt; &lt;p&gt;In a terminal, clone the repository and change into the directory installed:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone git@github.com:nodeshift-starters/nodejs-circuit-breaker.git $ cd nodejs-circuit-breaker&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before deploying your application, &lt;a href="https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html#cli-logging-in_cli-developer-commands"&gt;log in to your OpenShift cluster&lt;/a&gt;. Logging in requires a token, which you can retrieve from the OpenShift user interface (UI) by clicking on &lt;strong&gt;Copy login command&lt;/strong&gt; from the user drop-down menu in the top right corner. This gives you a command similar to:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc login --token=$TOKEN --server=$SERVER:6443&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After logging in, run the deployment script to deploy the application to OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./start-openshift.sh&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Deployment takes a few minutes. You can check progress from the Topology overview in the &lt;strong&gt;Developer&lt;/strong&gt; console. Once the services are deployed, you can start viewing your logs.&lt;/p&gt; &lt;h2&gt;Viewing the Node.js logs&lt;/h2&gt; &lt;p&gt;To view your logs, first set up a Kibana instance as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Inside the OpenShift UI, click the nine squares at the top right and then select logging.&lt;/li&gt; &lt;li&gt;Accept the permissions required by the service account.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;This takes you to your Kibana page, where you have to do a few things before viewing data.&lt;/p&gt; &lt;p&gt;The first task is to set up an index pattern so you can view the data. Enter "&lt;code&gt;app-nodejs*&lt;/code&gt;" for the pattern. Thanks to the trailing asterisk, the pattern allows you to view all logs from any application that uses "&lt;code&gt;nodejs&lt;/code&gt;" in its naming convention for its pods. The prepended string "&lt;code&gt;app&lt;/code&gt;" is from the &lt;code&gt;ClusterLogForwarder&lt;/code&gt;, to indicate that this index came from an application pod.&lt;/p&gt; &lt;p&gt;Select &lt;strong&gt;Timestamp&lt;/strong&gt; as the time filter field.&lt;/p&gt; &lt;p&gt;That's all you need to retrieve the logs.&lt;/p&gt; &lt;p&gt;Now, select &lt;strong&gt;Discover&lt;/strong&gt; at the top left, which displays all the logs inside your Elasticsearch instance. Here, you can filter through all the logs and look for specific logs from certain pods.&lt;/p&gt; &lt;p&gt;Because the index pattern I've suggested here matches logs from indexes belonging to my "nodejs" apps, I only have three logs, as shown in Figure 1. If I go down the left-hand side and select all the "structured." fields, the display shows only the parsed JSON in my Kibana results. These are the fields you can search on, making the most of your JSON logging.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/logs.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/logs.png?itok=4zXdkVKK" width="1440" height="721" alt="An example of the Kibana output showing only logs from three Node.js applications." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Kibana output, showing the logs selected by filtering for Node.js applications. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article was an introduction to using OpenShift's built-in cluster logging to consume Pino logs from your Node.js applications. We installed both the Elasticsearch Operator and the OpenShift Logging Operator, then deployed the OpenShift default Elasticsearch service and a custom &lt;code&gt;ClusterLogForwarder&lt;/code&gt;, all of which enabled us to collate all of our application logs.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/28/consuming-pino-logs-nodejs-applications" title="Consuming Pino logs from Node.js applications"&gt;Consuming Pino logs from Node.js applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/5-kcUwB5T4E" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ash Cripps</dc:creator><dc:date>2021-10-28T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/28/consuming-pino-logs-nodejs-applications</feedburner:origLink></entry><entry><title type="html">Keycloak.X Update</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cisB8OHj4Eg/keycloak-x-update" /><author><name>Stian Thorgersen</name></author><id>https://www.keycloak.org/2021/10/keycloak-x-update</id><updated>2021-10-28T00:00:00Z</updated><content type="html">It’s been quite some time since we announced the plans around Keycloak.X, two years in fact. Due to other priorities we’ve been a bit distracted, but now it’s finally full speed ahead. Keycloak.X will be lighter, faster, easier, more scalable, more cloud native, and a bunch of other things. Expect greatness coming your way! As part of Keycloak.X we’re not only making code changes, but there will also be a cultural shift where the team behind Keycloak will focus a lot more on user experience and the delivery of a manageable solution over simply pieces of code. There will be some disruptive changes coming, but we will strive to make the transition as easy as possible for everyone. For breaking changes such as moving from WildFly to Quarkus we plan to provide 6 months to do the migration. If that is not enough there is , which is a supported build of Keycloak by Red Hat. 7, which is based on current Keycloak architecture, has support until (currently says 2023, but will soon be extended to 2024). We will follow-up to this blog post with more details in the future, but for now let’s look at some of the highlights coming to Keycloak.X. HIGHLIGHTS EXPERIENCES As mentioned previously a lot more attention will be put on your experience with Keycloak. With this in mind we have identified a few experiences that we believe cover a wide range of different use-cases: * App developer Developers that are integrating Keycloak with applications and services * Customizer Developers that are extending Keycloak or integrating with other systems * Bridge Using Keycloak as a bridge between applications and other identity solutions * Regular A typical small to medium-sized deployment of Keycloak * Super-sized Elastic and highly available deployment of Keycloak for very large use-cases * SaaS A extension to super-sized where Keycloak enables identities for SaaS, CIAM, and B2C scenarios QUARKUS We’re switching to Quarkus as the platform to build Keycloak. Compared to WildFly this gives faster startup-time and lower memory footprint. It also provides a much simpler approach to configuring Keycloak, with command-line arguments and environment variables instead of complicated XML files. Another great aspect of Quarkus is that it gives us a lot more control over what external libraries are included in the distribution, including faster upgrades of dependencies, which should significantly improve on situation around CVEs. STORAGE RE-ARCHITECTURE We’re doing a significant re-architecture of the storage layer as part of Keycloak.X to address a number of shortcomings that where discovered in the current architecture. Zero downtime upgrade, scalability, and availability will be key topics of this new architecture, as well as making it a lot easier to support additional storage types in the long run. OPERATOR AND CONTAINERS With the current approach to configuration in Keycloak creating a good experience around a container is problematic as the container has to convert from environment variables to complicated XML configuration files. With the work we’re doing around Quarkus configuring Keycloak with environment variables becomes a native thing, making it a lot simpler to provide a great container experience. Similarly, the Operator can also be made simpler as it will be easier to configure Keycloak, as well as having better opinionated configuration from the base distribution, which trickles through from the Zip distribution, to the container, and finally to the Operator. To align the codebase more we’re also re-writing the Operator from scratch using the Java SDK and Quarkus. OBSERVABILITY Metrics, tracing, logging, and health-checks are all important aspect of a cloud native application. These are all important capabilities to manage and debug Keycloak in production, especially when running on Kubernetes or OpenShift. GITOPS FRIENDLY CONFIGURATION In a GitOps or CI/CD environment it can be problematic to manage the runtime configuration within Keycloak. As all configuration such as realms and clients live in the database and can only be managed through REST APIs it is hard to reliably manage as part of a GitOps process. Along with the storage re-architecture comes a very powerful capability that can federate configuration from multiple sources, and we plan to take advantage of this with a file-based store, where Keycloak can read more static/immutable configuration from the file-system (YAML of course), and combine this with dynamic/mutable configuration from the DB. Further, this enables checking in your static configuration in a Git repository, and deploy it to your development, stage and production environments as needed. EXTERNAL INTEGRATIONS Keycloak has a large number of extension points today, called SPIs. With Java (and in some cases JavaScript) it is possible to customize Keycloak with custom providers for these SPIs. Although, highly powerful and flexible, this is not ideal in a modern Kuberetes centric architecture. As the extensions are co-located with Keycloak it is harder to deploy, upgrade, and scale extensions. Extensions can also not be written in any language or framework making it more costly for non-Java developers to extend Keycloak. With this in mind we are planning more focus on the ability to extend and integrate with Keycloak through remote extensions, and are looking at REST, gRPC, Knative, Kafka, etc. as vehicles to achieve this. In addition we would also like to get to a point where we can have a "headless" Keycloak allowing a frontend to be built in any way you want, which would bring a great addition to the current themes approach to customising the UI. DECOMPOSING Last, but not least. We are also planning on ability to decompose Keycloak as well as bring better isolation on Keycloak’s code base and capabilities. We’re not planning to go full micro-service architecture here, but rather a sensible compromise allowing everything to run as a single process, with the ability to separate some parts of Keycloak into external services. ROADMAP As you can imagine all of what we have planned in Keycloak.X is a large amount of work, and won’t happen overnight. We’re focusing first on the breaking changes such as moving to Quarkus and re-architecture of the storage layer. Everything is not planned fully at this point, but we do have some idea of when we believe the various components of Keycloak.X will be delivered. * ASAP: Keycloak 16 will be the last preview of the Quarkus distribution, so we welcome everyone to try it out, and provide us with . * December 2021: In Keycloak 17 we will make the Quarkus distribution fully supported, and deprecate the WildFly distribution. * March 2022: In Keycloak 18 we are aiming to include the new Operator, and preview the new store. We’re also planning on removing WildFly support from the code-base at this point. * June 2022: First release with only the Quarkus distribution. We’re also hoping to make the new store a fully supported option at this point. The dates above are subject to change! FEEDBACK We would love your feedback on our plans around Keycloak.X, so please join us on to discuss the future of Keycloak!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cisB8OHj4Eg" height="1" width="1" alt=""/&gt;</content><dc:creator>Stian Thorgersen</dc:creator><feedburner:origLink>https://www.keycloak.org/2021/10/keycloak-x-update</feedburner:origLink></entry><entry><title type="html">Infinispan Operator 2.2.0.Final</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cJ2ua3v7aQc/infinispan-operator-2-2-final" /><author><name>Ryan Emerson</name></author><id>https://infinispan.org/blog/2021/10/27/infinispan-operator-2-2-final</id><updated>2021-10-27T12:00:00Z</updated><content type="html">We’re pleased to announce for Kubernetes and Red Hat OpenShift. This is the first Operator release based on Infinispan 13. Release highlights: * Custom server configuration. Add custom configuration for Infinispan Server using ConfigMap objects. * Configurable number of relay nodes for cross-site replication. Relay nodes send and receive replication requests from backup locations. You can now increase the number of relay nodes with the sites.local.maxRelayNodes field to achieve a better distribution of cross-site replication requests. * TLS security for cross-site replication traffic. You can now encrypt cross-site connections between Infinispan clusters with TLS by adding keystore secrets and configuring the sites.local.encryption field. * Operator SDK upgraded to v1.3.2&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cJ2ua3v7aQc" height="1" width="1" alt=""/&gt;</content><dc:creator>Ryan Emerson</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/10/27/infinispan-operator-2-2-final</feedburner:origLink></entry><entry><title>A compiler option, a hidden visibility, and a weak symbol walk into a bar</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CWaxrPz8bYg/compiler-option-hidden-visibility-and-weak-symbol-walk-bar" /><author><name>Serge Guelton</name></author><id>d1cfaebe-31ed-4e63-add8-743f9a855e39</id><updated>2021-10-27T07:00:00Z</updated><published>2021-10-27T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://llvm.org/"&gt;LLVM&lt;/a&gt; packaging team recently ran into a &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;compiler&lt;/a&gt; problem. A build of the LLVM package with &lt;a href="https://clang.llvm.org/"&gt;Clang&lt;/a&gt;, with link-time optimization activated, failed validation. This article steps through how we explored, identified, and ultimately fixed the problem.&lt;/p&gt; &lt;h2&gt;The LLVM package build&lt;/h2&gt; &lt;p&gt;The build on the &lt;a href="https://fedoraproject.org/wiki/Releases/Rawhide"&gt;Fedora Rawhide version&lt;/a&gt; should be an easy task at the packaging level:&lt;/p&gt; &lt;pre&gt;&lt;code class="diff"&gt;diff --git a/llvm.spec b/llvm.spec (...) @@ -1,3 +1,5 @@ +%global toolchain clang (...) +BuildRequires: clang &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let's introduce the characters in the mystery to follow.&lt;/p&gt; &lt;h2&gt;Disabling runtime type information&lt;/h2&gt; &lt;p&gt;By default, LLVM compiles with the &lt;code&gt;-fno-rtti&lt;/code&gt; option, which disables runtime type information. According to the LLVM coding standard, the compiler disables the information to &lt;a href="https://llvm.org/docs/CodingStandards.html#do-not-use-rtti-or-exceptions"&gt;reduce code and executable size&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Yet, sometimes, a type must be associated with a unique identifier. One example involves &lt;code&gt;llvm::Any&lt;/code&gt;, the LLVM version of &lt;code&gt;std::any&lt;/code&gt;. A typical implementation of &lt;code&gt;std::any&lt;/code&gt; involves &lt;code&gt;typeid&lt;/code&gt;, as showcased by &lt;a href="https://github.com/llvm/llvm-project/blob/6adbc83ee9e46b476e0f75d5671c3a21f675a936/libcxx/include/any#L293"&gt;the &lt;code&gt;libcxx&lt;/code&gt; version&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;typeid&lt;/code&gt; operator cannot be used with the &lt;code&gt;-fno-rtti&lt;/code&gt; option, so we had to find an alternative. The current implementation of &lt;code&gt;llvm::Any&lt;/code&gt; and &lt;code&gt;std::any&lt;/code&gt; can be mocked by the following snippet, mapping &lt;code&gt;&amp;TypeId&lt;MyType&gt;::Id&lt;/code&gt; to a unique identifier:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;template &lt;typename T&gt; struct TypeId { static const char Id; }; &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Hidden visibility&lt;/h2&gt; &lt;p&gt;Parts of LLVM are compiled with &lt;code&gt;-fvisibility=hidden&lt;/code&gt;. This option forces the default visibility of all symbols to be &lt;code&gt;hidden&lt;/code&gt;, which prevents them from being visible across library boundaries. Hiding symbols offers better control over exported symbols in a shared library.&lt;/p&gt; &lt;p&gt;What happens when the &lt;code&gt;TypeId&lt;/code&gt; construct from the previous section is combined with hidden visibility? Let's compile two shared libraries out of the same code:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;template &lt;typename T&gt; struct TypeId { static const char Id; }; template &lt;typename T&gt; const char TypeId&lt;T&gt;::Id = 0; #ifdef FOO const char* foo() { return &amp;TypeId&lt;int&gt;::Id; } #else const char* bar() { return &amp;TypeId&lt;int&gt;::Id; } #endif &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We compile two binaries, one with &lt;code&gt;FOO&lt;/code&gt; defined and one without, to carry out the different &lt;code&gt;#ifdef&lt;/code&gt; paths:&lt;/p&gt; &lt;pre&gt;&lt;code class="sh"&gt;&gt; clang++ -DFOO foo.cpp -shared -o libfoo.so &gt; clang++ foo.cpp -shared -o libbar.so &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Without hidden visibility, both libraries place our &lt;code&gt;Id&lt;/code&gt; at the same address:&lt;/p&gt; &lt;pre&gt;&lt;code class="sh"&gt;&gt; llvm-nm -C libfoo.so (...) 0000000000000679 V TypeId&lt;int&gt;::Id &gt; llvm-nm -C libbar.so (...) 0000000000000679 V TypeId&lt;int&gt;::Id &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;V&lt;/code&gt; in the output indicates that the symbol is a weak object, which means that only one of the items will be chosen by the linker. Therefore, we keep unicity of the symbol and its address across compilation units.&lt;/p&gt; &lt;p&gt;But when compiled with &lt;code&gt;-fvisibility-hidden&lt;/code&gt;, the symbols no longer are weak:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;&gt; clang++ -fvisibility=hidden foo.cpp -shared -o libbar.so &gt; llvm-nm -C libfoo.so (...) 0000000000000629 r TypeId&lt;int&gt;::Id &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;r&lt;/code&gt; next to the address means that the symbol is in a read-only data section. The symbol is not dynamically linked (as we can confirm from the output of &lt;code&gt;llvm-nm -D&lt;/code&gt;), so it gets different addresses in &lt;code&gt;libfoo&lt;/code&gt; and &lt;code&gt;libbar&lt;/code&gt;. In short, unicity is not preserved.&lt;/p&gt; &lt;h2&gt;Fine-grained control over symbol visibility&lt;/h2&gt; &lt;p&gt;A straightforward fix for the incompatibility we've uncovered is to explicitly state that &lt;code&gt;TypeId::Id&lt;/code&gt; must always have the default visibility. We can make this change as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;template &lt;typename T&gt; struct __attribute__((visibility("default"))) TypeId { static const char Id; }; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's check that the fix works:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;&gt; clang++ -fvisibility=hidden foo.cpp -shared -o libbar.so &gt; llvm-nm -C libfoo.so (...) 0000000000000659 V TypeId&lt;int&gt;::Id &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;V&lt;/code&gt; for a weak symbol has returned, but that's not the end of the story.&lt;/p&gt; &lt;p&gt;Instead of parameterizing &lt;code&gt;TypeId&lt;/code&gt; by &lt;code&gt;int&lt;/code&gt;, let's parameterize it by a &lt;code&gt;HiddenType&lt;/code&gt; class declared with hidden visibility:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;struct HiddenType {}; template &lt;typename T&gt; struct __attribute__((visibility("default"))) TypeId { static const char Id; }; template &lt;typename T&gt; const char TypeId&lt;T&gt;::Id = 0; const char* foo() { return &amp;TypeId&lt;HiddenType&gt;::Id; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When compiling this code with &lt;code&gt;-fvisibility-hidden&lt;/code&gt;, where does &lt;code&gt;TypeId&lt;HiddenType&gt;::Id&lt;/code&gt; end up?&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;&gt; clang++ -fvisibility=hidden foo.cpp -shared -o libbar.so &gt; llvm-nm -CD libbar.so | grep -c TypeId&lt;HiddenType&gt;::Id 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Fascinating! This exercise shows that a template function with &lt;em&gt;default&lt;/em&gt; visibility, instantiated with a type of &lt;em&gt;hidden&lt;/em&gt; visibility, ends up with &lt;em&gt;hidden&lt;/em&gt; visibility. Indeed, flagging &lt;code&gt;HiddenType&lt;/code&gt; with &lt;code&gt;__attribute__((visibility("default")))&lt;/code&gt; restores the expected behavior.&lt;/p&gt; &lt;h2&gt;Where theory meets LLVM&lt;/h2&gt; &lt;p&gt;Once we isolated the behavior described in the preceding section, we could easily provide the relevant patches in LLVM:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://reviews.llvm.org/D101972"&gt;Force visibility of &lt;code&gt;llvm::Any&lt;/code&gt; to external&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://reviews.llvm.org/D108943"&gt;Fine grain control over some symbol visibility&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://reviews.llvm.org/D109252"&gt;Add extra check for &lt;code&gt;llvm::Any::TypeId&lt;/code&gt; visibility&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;These patches fix the build issue mentioned at the beginning of the article and ensure that it won't reproduce, which is the kind of outcome programmers always look for.&lt;/p&gt; &lt;h2&gt;Acknowledgments&lt;/h2&gt; &lt;p&gt;The author would like to thank Béatrice Creusillet, Adrien Guinet, and the editorial team for their help on this article.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/27/compiler-option-hidden-visibility-and-weak-symbol-walk-bar" title="A compiler option, a hidden visibility, and a weak symbol walk into a bar"&gt;A compiler option, a hidden visibility, and a weak symbol walk into a bar&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CWaxrPz8bYg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Serge Guelton</dc:creator><dc:date>2021-10-27T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/27/compiler-option-hidden-visibility-and-weak-symbol-walk-bar</feedburner:origLink></entry><entry><title>Quarkus 2.4.0.Final released - Hibernate Reactive 1.0.0, Kafka Streams DevUI, Multi module continuous testing, AWT image resize via new AWT extension and much more</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/MNE00QjR5gQ/" /><author><name>Alexey Loubyansky (https://twitter.com/aloubyansky)</name></author><id>https://quarkus.io/blog/quarkus-2-4-0-final-released/</id><updated>2021-10-27T00:00:00Z</updated><published>2021-10-27T00:00:00Z</published><summary type="html">Today, we release Quarkus 2.4.0.Final which includes a lot of refinements and improvements and some new features: Hibernate Reactive 1.0.0.Final Introducing Kafka Streams DevUI Support continuous testing for multi module projects Support AWT image resize via new AWT extension Migration Guide To migrate from 2.3, please refer to our migration...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/MNE00QjR5gQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Alexey Loubyansky (https://twitter.com/aloubyansky)</dc:creator><dc:date>2021-10-27T00:00:00Z</dc:date><feedburner:origLink>
                https://quarkus.io/blog/quarkus-2-4-0-final-released/
            </feedburner:origLink></entry><entry><title type="html">Counterfactuals; getting the right answer</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aspIGNLHIXg/counterfactuals-getting-the-right-answer.html" /><author><name>Roberto Emanuel</name></author><id>https://blog.kie.org/2021/10/counterfactuals-getting-the-right-answer.html</id><updated>2021-10-26T13:54:07Z</updated><content type="html">Sometimes the result of an automated decision may be neither desired or that which was required. What if there was a tool to find a way to overturn those decisions, maybe changing some of the figures that were provided to the system, and achieve a different outcome? That’s what we’ve been working on lately within the Trusty AI initiative. We added a new experimental feature to the Audit Investigation console called Counterfactual Analysis. Counterfactuals allow for the outcome of a decision to be set and a range of viable inputs to be searched over until the outcome is achieved. Let’s see how it works in detail with a practical example. GETTING AN APPROVAL FOR A DENIED LOAN We will consider a model responsible to approve or deny mortgage applications. The model produces two outputs, specifically Mortgage Approval (obviously) and a Risk Score associated to the loan request. We have submitted a request to the model, so we go to the Audit investigation dashboard and we open the execution detail to discover that the mortgage request was denied. Let’s open the Counterfactual tool using the new tab available in the navigation menu. At this point we want to specify a different outcome for the decision. To do so, we click on the "Set Up Outcomes" button. A modal window will open where we can see the original outcomes and we can change them according to our desire. We’ll set "Mortgage Approval" to True and we’ll check "Automatically adjust for counterfactual" for the Risk score. In this way the analysis will search for any risk score value that produces a Mortgage approval. Then we’ll click on "Confirm". Now we have to specify how we would like to alter the inputs in order to achieve the outcomes we’ve just chosen. In the CF table we see the list of all the inputs provided within the execution and their original value. Clicking on the checkbox at the beginning of an input row will set the input as changeable. Then clicking on its "Constraint" button we will have to set a range of values for the selected input. The system will search for values within the provided range to find solutions matching the desired outcome. So, we will enable the "TotalRequired" input and we’ll set up a range constraint from 0 to 100,000. We are basically searching what’s the amount that could be loaned given the provided salary and assets. At this point we’ve filled out all the required information to run the analysis. We’ve selected at least one outcome different from the original one (Mortgage Approval, from false to true) and we provided at least one searchable input (the requested amount). RUNNING THE ANALYSIS We are all set and we can click on "Run analysis" to start looking for solutions. The default analysis will run for one minute. At the end of it we can see that the system was able to find some results! The best solution found is showed at the far left of the results area and it’s marked with a star near its ID. We can see that the mortgage request could be approved for a "TotalRequired" sum around 18,500. At this point we could try another run by allowing other inputs to change. By clicking on "Edit Counterfactual" it’s possible to start over keeping the search options already provided. We could eventually try searching with higher values for "TotalAsset" or "MonthlySalary" for example. That’s it! Our brief introduction to the Counterfactual Analysis ends here. Keep in mind that it is still an experimental feature at the moment. It only supports a limited set of types for the outcomes and the inputs of a decision (only numbers and booleans). The Counterfactual Analysis tool will be available with the upcoming release of Kogito 1.13. You can learn more about running TrustyAI in the . FURTHER READINGS If you are interested in reading more about how counterfactuals work behind the hood you can explore the following resources: * * * The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aspIGNLHIXg" height="1" width="1" alt=""/&gt;</content><dc:creator>Roberto Emanuel</dc:creator><feedburner:origLink>https://blog.kie.org/2021/10/counterfactuals-getting-the-right-answer.html</feedburner:origLink></entry><entry><title>Configuring Java applications to use Cryostat</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/f3NJRLPv-VA/configuring-java-applications-use-cryostat" /><author><name>Andrew Azores</name></author><id>0a0f7b21-9dcd-4bd5-a68a-1d3c2594154e</id><updated>2021-10-26T07:00:00Z</updated><published>2021-10-26T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/25/introduction-to-containerjfr-jdk-flight-recorder-for-containers"&gt;Cryostat&lt;/a&gt; is a profiling and monitoring tool that leverages the &lt;a href="https://developers.redhat.com/blog/2020/08/25/get-started-with-jdk-flight-recorder-in-openjdk-8u"&gt;JDK Flight Recorder&lt;/a&gt; (JFR) framework already present in your HotSpot JVM applications. Cryostat provides an in-cluster collection hub for easy and secure access to your JDK Flight Recorder data from outside of the cluster.&lt;/p&gt; &lt;p&gt;This article follows our &lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;recent announcement of Cryostat 2.0&lt;/a&gt;. It is the first of several hands-on guides to using Cryostat 2.0 in your Java applications. In this article, we'll explore how to set up and configure a &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;-based Java application to use Cryostat on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href="https://access.redhat.com/documentation/en-us/openjdk/11/html/release_notes_for_cryostat_2.0"&gt;The Red Hat build of Cryostat 2.0&lt;/a&gt; is now widely available in technology preview. Cryostat 2.0 introduces many new features and improvements, such as automated rules, a better API response JSON format, custom targets, concurrent target JMX connections, WebSocket push notifications, and more. The Red Hat build includes the &lt;a href="https://catalog.redhat.com/software/operators/detail/60ee049a744684587e218ef5"&gt;Cryostat Operator&lt;/a&gt; to simplify and automate Cryostat deployment on OpenShift.&lt;/p&gt; &lt;h2&gt;JMX and Cryostat&lt;/h2&gt; &lt;p&gt;The main prerequisite for using Cryostat is that your application must have Java Management Extensions (JMX) enabled and exposed. In OpenShift, exposing the JMX port means either using the Cryostat-default port number of &lt;code&gt;9091&lt;/code&gt; or naming the port &lt;code&gt;jfr-jmx&lt;/code&gt;. We will explore the procedure for exposing JMX with a Quarkus-based Java application deployed on OpenShift.&lt;/p&gt; &lt;h2&gt;Step 1: Generate the Quarkus sample application&lt;/h2&gt; &lt;p&gt;Let’s get started with the Quarkus sample application. To begin, we will visit &lt;a href="https://code.quarkus.io/"&gt;code.quarkus.io&lt;/a&gt; and generate a new application. For the purpose of this article, we will use the default &lt;code&gt;org.acme&lt;/code&gt; group and &lt;code&gt;code-with-quarkus&lt;/code&gt; &lt;code&gt;artifactId&lt;/code&gt;. Feel free to customize these as you see fit. Once you have set up your generation options, download and extract the .zip to a working directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ mv Downloads/code-with-quarkus.zip workspace $ cd workspace $ unzip code-with-quarkus.zip $ cd code-with-quarkus &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we have our Quarkus application generated and ready for modification. Let’s do a quick sanity check before continuing. Once &lt;code&gt;mvnw&lt;/code&gt; has downloaded Quarkus' dependencies and built the application, you should see a message something like: “Listening on: http://localhost:8080.” Visit this URL to ensure that you see the Quarkus default welcome page.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ ./mvnw compile quarkus:dev &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 2: Configure the application for JMX&lt;/h2&gt; &lt;p&gt;If everything looks good so far, we can now set up the Quarkus application for JMX. Switch back to the terminal running your Quarkus application and press &lt;strong&gt;Ctrl-C&lt;/strong&gt; to stop the dev server.&lt;/p&gt; &lt;p&gt;Now that the dev server is stopped we will go ahead and edit the project's &lt;code&gt;Dockerfile.jvm&lt;/code&gt;. This file contains directives for Open Container Initiative (OCI) image builders such as Podman, Buildah, and Docker to follow when assembling the Quarkus application into an OCI image (specifically, when the application is built in JVM mode as opposed to native image mode). Once we have our JVM-mode application packaged into an OCI image then we can deploy and run that image as a container in OpenShift or Podman. Let's continue and edit the Dockerfile:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ $VISUAL src/main/docker/Dockerfile.jvm &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There should be a line like this one:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-dockerfile"&gt; ENV JAVA_OPTIONS=”-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager” &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let’s modify this line and add options to enable JMX:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-dockerfile"&gt; ENV JAVA_OPTIONS="-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Dcom.sun.management.jmxremote.port=9096 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here, we have enabled JMX on port 9096. For the purpose of this article, I have disabled JMX SSL and JMX authentication. In production, both should be enabled (see the &lt;a href="https://docs.oracle.com/javadb/10.10.1.2/adminguide/radminjmxenablepwdssl.html"&gt;Oracle documentation&lt;/a&gt; for more about JMX administration).&lt;/p&gt; &lt;p&gt;Now, we need to make one last change to the Dockerfile: The line &lt;code&gt;EXPOSE 8080&lt;/code&gt; should become &lt;code&gt;EXPOSE 8080 9096&lt;/code&gt;. This will add metadata to the OCI image hinting that the application within will listen on ports &lt;code&gt;8080&lt;/code&gt; and &lt;code&gt;9096&lt;/code&gt;, so that when we deploy this on OpenShift later those two ports will be automatically included in the generated Service for intra-cluster network traffic.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Why we're running Quarkus in JVM mode&lt;/strong&gt;: You may have noticed that we have edited only the Dockerfile.jvm and not the other Dockerfiles, like Dockerfile.native. This is because Quarkus in native-image mode does not currently support JMX. We will need to run Quarkus in JVM mode to have access to JMX and, therefore, to be Cryostat-compatible.&lt;/p&gt; &lt;h2&gt;Step 3: Take it for a test drive (optional)&lt;/h2&gt; &lt;p&gt;This configuration will set up the sample Quarkus application to use JMX when built and run as an OCI image. If you would like to test drive JMX and JFR with Quarkus before we get to that stage, you can do the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ ./mvnw -Djvm.args=”-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Dcom.sun.management.jmxremote.port=9096 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false” compile quarkus:dev &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will run the &lt;code&gt;quarkus:dev&lt;/code&gt; server with JMX enabled and configured the same way we have specified in the Dockerfile.jvm. Now, open JDK Mission Control, and you should see the Quarkus application in the JVM browser panel.&lt;/p&gt; &lt;h2&gt;Step 4: Build the application into a container image&lt;/h2&gt; &lt;p&gt;Once we are satisfied with our configuration, let’s go ahead and build the application into a container image. Since we want to deploy this application to OpenShift, I will tag it with an example &lt;code&gt;quay.io&lt;/code&gt; tag, which you should replace with your own username and application name:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ ./mvnw package $ podman build -f src/main/docker/Dockerfile.jvm -t quay.io/&lt;namespace&gt;/code-with-quarkus . &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We'll do one more sanity check, and if it looks good, push it to quay.io:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ podman run -i --rm -p 8080:8080 -p 9096:9096 quay.io/&lt;namespace&gt;/code-with-quarkus $ # open http://localhost:8080 in your browser once more and ensure you see the Quarkus welcome page $ podman push quay.io/&lt;namespace&gt;/code-with-quarkus &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once that image is pushed we need to visit quay.io (for example, https://quay.io/&lt;namespace&gt;/code-with-quarkus?tab=settings) and make the image repository public. After that, we can deploy it to our OpenShift cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ oc new-app quay.io/&lt;namespace&gt;/code-with-quarkus:latest $ oc edit svc code-with-quarkus $ # rename the port “9096-tcp” to “jfr-jmx”, but leave the port, targetPort, etc. the same $ oc expose --port=8080 svc code-with-quarkus $ oc status # check that the code-with-quarkus app is accessible and still displays the Quarkus welcome page &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 5: Install Cryostat 2.0 on OpenShift&lt;/h2&gt; &lt;p&gt;See the &lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;Cryostat 2.0 announcement&lt;/a&gt; for how to install Cryostat in your OpenShift cluster using the Cryostat Operator. Once you have Cryostat installed and present in the same OpenShift namespace as your code-with-quarkus example application, you can verify that everything is wired up correctly:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ oc get flightrecorders &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There should be an item like &lt;code&gt;code-with-quarkus-5645dbdd47-z7pl7&lt;/code&gt;. This indicates that the Cryostat Operator is running and recognizes our Quarkus application as being Cryostat-compatible.&lt;/p&gt; &lt;p&gt;Check &lt;code&gt;oc status&lt;/code&gt; again and visit the Cryostat URL. Enter your OpenShift account token (you can retrieve it from the OpenShift console or with &lt;code&gt;oc whoami -t&lt;/code&gt;), then select the &lt;code&gt;code-with-quarkus application&lt;/code&gt; and go to the &lt;strong&gt;Events&lt;/strong&gt; view. If you see a list of event templates loaded, then your Cryostat and Quarkus instances are communicating successfully!&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article was the first of several that will provide hands-on introductions to using Cryostat with your Java applications. Look for the next article in this series, which introduces you to defining custom targets for Cryostat.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/26/configuring-java-applications-use-cryostat" title="Configuring Java applications to use Cryostat"&gt;Configuring Java applications to use Cryostat&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/f3NJRLPv-VA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Andrew Azores</dc:creator><dc:date>2021-10-26T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/26/configuring-java-applications-use-cryostat</feedburner:origLink></entry><entry><title type="html">Kogito 1.12.0 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jCSVrBT_zuY/kogito-1-12-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2021/10/kogito-1-12-0-released.html</id><updated>2021-10-26T00:48:26Z</updated><content type="html">We are glad to announce that the Kogito 1.12.0 release is now available! This goes hand in hand with, , Operator, and CLI 1.12.0 release. From a feature point of view, we included a series of new features and bug fixes, including: * Improvement in protobuf persistence generation: support for fields inherited from parent classes and ability to disable protobuf generation * Ability to create custom REST endpoints using Kogito incubator API * Data Index service new Gateway API methods for handling process instances, human tasks and jobs * JDBC persistence addon now has been tested with Oracle and should work with any ANSI SQL compatible storage (thanks for the contribution) * Kogito event driven DRL: ability to trigger DRL evaluation via CloudEvent Kafka messages * Kogito event driven PMML: ability to trigger PMML evaluation via CloudEvent Kafka messages * Support for FEEL expression in BPMN Gateways * Added BPMN validations around data input and and output types where they should match the expected type from the target/source process variable. BREAKING CHANGES * Operator OLM installation, today we support OwnNamespace and SingleNamespace install modes, however, when using OLM, AllNamesapces mode will be used.  For more details head to the complete. All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found. * Kogito images are available on. * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.13.0 artifacts are available at the. A detailed changelog for 1.12.0 can be found in. New to Kogito? Check out our website. Click the "Get Started" button. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jCSVrBT_zuY" height="1" width="1" alt=""/&gt;</content><dc:creator>Cristiano Nicolai</dc:creator><feedburner:origLink>https://blog.kie.org/2021/10/kogito-1-12-0-released.html</feedburner:origLink></entry><entry><title type="html">What's new in Vert.x 4.2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/URg6OKCDNRc/whats-new-in-vert-x-4-2" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/whats-new-in-vert-x-4-2</id><updated>2021-10-26T00:00:00Z</updated><content type="html">See an overview of all new and exciting features in Vert.x 4.2, including Java 17 supports, Vert.x Oracle Client and more.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/URg6OKCDNRc" height="1" width="1" alt=""/&gt;</content><dc:creator>Julien Viet</dc:creator><feedburner:origLink>https://vertx.io/blog/whats-new-in-vert-x-4-2</feedburner:origLink></entry><entry><title type="html">Secure WildFly applications with OpenID Connect</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/AeRoyUibUok/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-security/secure-wildfly-applications-with-openid-connect/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=secure-wildfly-applications-with-openid-connect</id><updated>2021-10-25T10:41:56Z</updated><content type="html">WildFly 25 enables you to secure deployments using OpenID Connect (OIDC) without installing a Keycloak client adapter. This tutorial will show a proof of concept example of it. OpenID Connect is a simple identity layer on top of the OAuth 2.0 protocol which allows Clients to verify the identity of the End-User based on the ... The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/AeRoyUibUok" height="1" width="1" alt=""/&gt;</content><dc:creator>F.Marchioni</dc:creator><feedburner:origLink>http://www.mastertheboss.com/jbossas/jboss-security/secure-wildfly-applications-with-openid-connect/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=secure-wildfly-applications-with-openid-connect</feedburner:origLink></entry></feed>
